{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means model\n",
    "## Netflix Rating Predictions\n",
    "#### Lauren Neal - ln9bv\n",
    "#### Melanie Sattler - ms9py\n",
    "#### Nick Thompson - nat3fa\n",
    "#### Nima Beheshti - nb9pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|movie_id|rating|\n",
      "+-------+--------+------+\n",
      "|1488844|       1|   3.0|\n",
      "| 822109|       1|   5.0|\n",
      "| 885013|       1|   4.0|\n",
      "|  30878|       1|   4.0|\n",
      "| 823519|       1|   3.0|\n",
      "+-------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml import Pipeline \n",
    "\n",
    "# Loads data\n",
    "df = spark.read.csv(\"processed_all.txt\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creat new variable that shows ratings 4+ as 1 and three of lower as 0. Will use this for metric evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|classification|\n",
      "+-------+--------+------+--------------+\n",
      "|1488844|       1|   3.0|             0|\n",
      "| 822109|       1|   5.0|             1|\n",
      "| 885013|       1|   4.0|             1|\n",
      "|  30878|       1|   4.0|             1|\n",
      "| 823519|       1|   3.0|             0|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('classification', f.when(f.col('rating') >= 4, 1).otherwise(0))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order the dataframe by user_id so we can test using all movies seen by a specific user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|classification|\n",
      "+-------+--------+------+--------------+\n",
      "|      6|    3290|   5.0|             1|\n",
      "|      6|    3423|   4.0|             1|\n",
      "|      6|    3315|   3.0|             0|\n",
      "|      6|    3226|   3.0|             0|\n",
      "|      6|    3320|   3.0|             0|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('df')\n",
    "df = spark.sql(\"SELECT * FROM df ORDER BY user_id\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find count of dataframe and limit observations to 500,000 rows due to time it takes to run test and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100480507"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = df.limit(500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 314\n",
    "weights = [.8, .2]\n",
    "(training, test) = use.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|classification|\n",
      "+-------+--------+------+--------------+\n",
      "|      6|      30|   3.0|             0|\n",
      "|      6|     157|   3.0|             0|\n",
      "|      6|     173|   4.0|             1|\n",
      "|      6|     191|   2.0|             0|\n",
      "|      6|     241|   3.0|             0|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399828"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100172"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add in additional dataset that provides some more info on the movies in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+-------+------+-------------------+--------------------+--------------------+--------------------+\n",
      "| id|  year|               title|Runtime|Rating|          Directors|             Writers|Production companies|              Genres|\n",
      "+---+------+--------------------+-------+------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  1|2003.0|     Dinosaur Planet|   50.0|   7.7|Pierre de Lespinois|Mike Carrol-Mike ...|                null|Documentary-Anima...|\n",
      "|  2|2004.0|Isle of Man TT 20...|   null|  null|               null|                null|                null|                null|\n",
      "|  3|1997.0|           Character|  122.0|   7.8|      Mike van Diem|Ferdinand Bordewi...|First Floor Featu...| Crime-Drama-Mystery|\n",
      "|  4|1994.0|Paula Abdul's Get...|   54.0|   8.8|      Steve Purcell|                null|                null|              Family|\n",
      "|  5|2004.0|The Rise and Fall...|  360.0|   8.6|         Kevin Dunn|         Paul Heyman|      WWE Home Video|   Documentary-Sport|\n",
      "+---+------+--------------------+-------+------+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_movie_feats = spark.read.csv(\"movies_addtl_features.csv\", header=True, inferSchema=True)\n",
    "add_movie_feats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "| id|  year|Runtime|\n",
      "+---+------+-------+\n",
      "|  1|2003.0|   50.0|\n",
      "|  2|2004.0|   null|\n",
      "|  3|1997.0|  122.0|\n",
      "|  4|1994.0|   54.0|\n",
      "|  5|2004.0|  360.0|\n",
      "+---+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = add_movie_feats.drop('Title', 'Rating', 'Directors', 'Writers', 'Production Companies', 'Genres')\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine columns for year and runtime of specific movie by movie_id. Drop NA values and duplicate columns for each the training and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_training = training.join(df2, training.movie_id == df2.id )\n",
    "combined_training = combined_training.dropna()\n",
    "combined_training = combined_training.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+------+-------+\n",
      "|user_id|movie_id|rating|classification|  year|Runtime|\n",
      "+-------+--------+------+--------------+------+-------+\n",
      "|      6|      30|   3.0|             0|2003.0|  128.0|\n",
      "|      6|     173|   4.0|             1|1968.0|  130.0|\n",
      "|      6|     191|   2.0|             0|2003.0|  134.0|\n",
      "|      6|     241|   3.0|             0|1959.0|  136.0|\n",
      "|      6|     295|   4.0|             1|1995.0|   90.0|\n",
      "+-------+--------+------+--------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+------+-------+\n",
      "|user_id|movie_id|rating|classification|  year|Runtime|\n",
      "+-------+--------+------+--------------+------+-------+\n",
      "|      6|     175|   5.0|             1|1992.0|   99.0|\n",
      "|      6|     197|   3.0|             0|2004.0|  103.0|\n",
      "|      6|     329|   4.0|             1|1999.0|  130.0|\n",
      "|      6|     723|   3.0|             0|1991.0|  101.0|\n",
      "|      6|     872|   3.0|             0|1954.0|  207.0|\n",
      "+-------+--------+------+--------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_test = test.join(df2, test.movie_id == df2.id )\n",
    "combined_test = combined_test.dropna()\n",
    "combined_test = combined_test.drop('id')\n",
    "combined_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "va0 = VectorAssembler(inputCols=[\"rating\"], outputCol=\"features\")\n",
    "va1 = VectorAssembler(inputCols=[\"rating\",\"year\",'Runtime'], outputCol=\"features\")\n",
    "va2 = VectorAssembler(inputCols=[\"year\", \"Runtime\"], outputCol=\"features\")\n",
    "va3 = VectorAssembler(inputCols=[\"rating\", \"Runtime\"], outputCol=\"features\")\n",
    "va4 = VectorAssembler(inputCols=[\"rating\", \"year\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")\n",
    "# sc1 = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")\n",
    "# sc2 = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")\n",
    "# sc3 = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")\n",
    "# sc4 = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = KMeans(featuresCol= 'scaled').setK(2).setSeed(314).setMaxIter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline0 = Pipeline(stages=[va0, sc, mod])\n",
    "pipeline1 = Pipeline(stages=[va1, sc, mod])\n",
    "pipeline2 = Pipeline(stages=[va2, sc, mod])\n",
    "pipeline3 = Pipeline(stages=[va3, sc, mod])\n",
    "pipeline4 = Pipeline(stages=[va4, sc, mod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test models using various feature selections to see which one performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 Score = 0.0\n"
     ]
    }
   ],
   "source": [
    "model0 = pipeline0.fit(combined_training)\n",
    "prediction0 = model0.transform(combined_test).select(\"classification\", \"prediction\")\n",
    "metrics = MulticlassMetrics(prediction0.rdd.map(lambda x: tuple(map(float, x))))\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.001748849176834784\n",
      "Recall = 0.002331814526936478\n",
      "F1 Score = 0.0019986905131120993\n"
     ]
    }
   ],
   "source": [
    "model1 = pipeline1.fit(combined_training)\n",
    "prediction1 = model1.transform(combined_test).select(\"classification\", \"prediction\")\n",
    "metrics = MulticlassMetrics(prediction1.rdd.map(lambda x: tuple(map(float, x))))\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.8469857478842945\n",
      "Recall = 0.5499432240886487\n",
      "F1 Score = 0.6668829729986389\n"
     ]
    }
   ],
   "source": [
    "model2 = pipeline2.fit(combined_training)\n",
    "prediction2 = model2.transform(combined_test).select(\"classification\", \"prediction\") \n",
    "metrics = MulticlassMetrics(prediction2.rdd.map(lambda x: tuple(map(float, x))))\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 Score = 0.0\n"
     ]
    }
   ],
   "source": [
    "model3 = pipeline3.fit(combined_training)\n",
    "prediction3 = model3.transform(combined_test).select(\"classification\", \"prediction\")\n",
    "metrics = MulticlassMetrics(prediction3.rdd.map(lambda x: tuple(map(float, x))))\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.09158341206504915\n",
      "Recall = 0.15891729742927901\n",
      "F1 Score = 0.11620077535196897\n"
     ]
    }
   ],
   "source": [
    "model4 = pipeline4.fit(combined_training)\n",
    "prediction4 = model4.transform(combined_test).select(\"classification\", \"prediction\")\n",
    "metrics = MulticlassMetrics(prediction4.rdd.map(lambda x: tuple(map(float, x))))\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Loop to find the best value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [2,5,10,15,20]\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1Score_list = []\n",
    "for k in k_list:\n",
    "    va5 = VectorAssembler(inputCols=[\"year\", \"Runtime\"], outputCol=\"features\")\n",
    "    sc5 = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")\n",
    "    mod = KMeans(featuresCol= 'scaled').setK(k).setSeed(314).setMaxIter(10)\n",
    "    pipeline5 = Pipeline(stages=[va5, sc5, mod])\n",
    "    model5 = pipeline5.fit(combined_training)\n",
    "    prediction5 = model5.transform(combined_test).select(\"classification\", \"prediction\") \n",
    "    metrics = MulticlassMetrics(prediction5.rdd.map(lambda x: tuple(map(float, x))))\n",
    "    precision = metrics.precision(1.0)\n",
    "    recall = metrics.recall(1.0)\n",
    "    f1Score = metrics.fMeasure(1.0)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1Score_list.append(f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe to show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+-------------------+\n",
      "|k_value|           Precision|            Recall|            F1Score|\n",
      "+-------+--------------------+------------------+-------------------+\n",
      "|      2|  0.8469857478842945|0.5499432240886487| 0.6668829729986389|\n",
      "|      5| 0.07027559450821155|0.6633776091081593|0.12708799098460477|\n",
      "|     10| 0.01692564375741251|0.6223207686622321|0.03295499021526419|\n",
      "|     15|  0.0179307294912256|0.7181964573268921|0.03498793857498676|\n",
      "|     20|0.047198826059862906| 0.662528216704289|0.08811994520650766|\n",
      "+-------+--------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = spark.createDataFrame(zip(k_list,precision_list,recall_list,f1Score_list), \n",
    "                                   [\"k_value\", \"Precision\", \"Recall\", \"F1Score\"])\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running models based on various combinations of feature columns, we noticed that 'rating' is not a good column to chose. Using rating in a real world recommendation setting would lead to major bias because the thought behind recommending a movie to someone they wouldn't have already seen it and therefore you wouldn't already have a rating associated with it. The model that seemed to perform the best was the model using only 'year' and 'Runtime' as features. We worked off of this model to see which k-value led to the best results and and it appears that the leader is k value of 2. We will dive further into the results and observations in our project write-up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
